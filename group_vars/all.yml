---
##########
# DOCKER #
##########
ceph_docker_image_tag: "14.2.6-nautilus-centos-7-x86_64"
######################
# Core Configuration #
######################
generate_fsid: false
fsid: "9aa94949-14cd-4ee8-b4bc-c0e4c0a64543"
public_network: "10.148.0.0/24"
cluster_network: "10.148.0.0/24"
monitor_address_block: "10.148.0.0/24"
radosgw_address_block: "10.148.0.0/24"
mon_use_fqdn: false
containerized_deployment: True
configure_firewall: false
ceph_conf_local: false
nfs_ganesha_stable: true # use stable repos for nfs-ganesha
nfs_ganesha_stable_branch: V2.7-stable
# ceph_conf_overrides: 
#   osd:
#     osd_op_num_shards: 64
#     osd_op_num_threads_per_shard: 1

###########################
# Resource Limit & Tuning #
###########################
# is_hci: true # Auto calculate osd_memory_target per OSD
# hci_safety_factor: 0.2  ## For HCI node
# non_hci_safety_factor: 0.8  ## For non HCI node (dedicated)
# ceph_mon_docker_cpu_limit: 1
# ceph_rgw_docker_cpu_limit: 2
# ceph_mds_docker_cpu_limit: 2
# ceph_osd_docker_cpu_limit: 1
# ceph_mgr_docker_cpu_limit: 1
# ceph_osd_docker_memory_limit: "{{ osd_memory_target * 1.5 }}" # cgroup limit docker 50% higher than osd_memory_target
# osd_memory_target: 6g # memory of node that want to use ceph
# The next two variables are undefined, and thus, unused by default.
# If `lscpu | grep NUMA` returned the following:
#  NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16
#  NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17
# then, the following would run the OSD on the first NUMA node only.
# ceph_osd_docker_cpuset_cpus: "12-15,32-47"
# ceph_osd_docker_cpuset_mems: "0" ## Numa node

## OSD options
#
osd_pool_default_pg_num: 8
osd_pool_default_size: 2
osd_pool_default_min_size: 1
#mds_max_mds: 1
devices:   
  - '/dev/vdb'

## Rados Gateway options
#
radosgw_frontend_type: beast # For additionnal frontends see: http://docs.ceph.com/docs/nautilus/radosgw/frontends/
radosgw_frontend_ssl_certificate: ""
radosgw_thread_pool_size: 512
radosgw_num_instances: 1
email_address: engineer@nipa.cloud


#####################
# Kolla-integration #
#####################
api_port: 5100
# ceph_mgr_packages:
#  - ceph-mgr
#  - ceph-mgr-dashboard
#  - ceph-mgr-diskprediction-local
ceph_mgr_modules:
  - alerts
  - diskprediction_local
  - restful
  - iostat

###############
# NFS-GANESHA #
###############

# Set this to true to enable File access via NFS.  Requires an MDS role.
nfs_file_gw: true
ceph_nfs_dynamic_exports: true
ceph_nfs_rados_backend: true
# Set this to true to enable Object access via NFS. Requires an RGW role.
nfs_obj_gw: true
#ceph_nfs_rgw_access_key: "QFAMEDSJP5DEKJO0DDXY"
#ceph_nfs_rgw_secret_key: "iaSFLDVvDdQt6lkNzHyW4fPLZugBAI1g17LO0+87[MAC[M#C"


## Client only docker image - defaults to {{ ceph_docker_image }}
#ceph_client_docker_image: "{{ ceph_docker_image }}"
#ceph_client_docker_image_tag: "{{ ceph_docker_image_tag }}"
#ceph_client_docker_registry: "{{ ceph_docker_registry }}"
#ceph_docker_enable_centos_extra_repo: false
#ceph_docker_on_openstack: false
#containerized_deployment: False
#container_binary:
#timeout_command: "{{ 'timeout --foreground -s KILL ' ~ docker_pull_timeout if (docker_pull_timeout != '0') and (ceph_docker_dev_image is undefined or not ceph_docker_dev_image) else '' }}"

#############
# DASHBOARD #
#############
dashboard_enabled: True
# Choose http or https
# For https, you should set dashboard.crt/key and grafana.crt/key
#dashboard_protocol: http
# dashboard_port: 8443
dashboard_admin_user: admin
dashboard_admin_password: p@ssw0rd
# We only need this for SSL (https) connections
#dashboard_crt: ''
#dashboard_key: ''
#dashboard_rgw_api_user_id: ceph-dashboard
#dashboard_rgw_api_admin_resource: ''
#dashboard_rgw_api_no_ssl_verify: False
#node_exporter_container_image: "prom/node-exporter:v0.17.0"
#node_exporter_port: 9100
grafana_admin_user: admin
grafana_admin_password: p@ssw0rd
# We only need this for SSL (https) connections
#grafana_crt: ''
#grafana_key: ''
#grafana_container_image: "grafana/grafana:5.2.4"
#grafana_container_cpu_period: 100000
#grafana_container_cpu_cores: 2
# container_memory is in GB
#grafana_container_memory: 4
#grafana_uid: 472
#grafana_datasource: Dashboard
#grafana_dashboards_path: "/etc/grafana/dashboards/ceph-dashboard"
#grafana_dashboard_version: nautilus
#grafana_dashboard_files:
#  - ceph-cluster.json
#  - cephfs-overview.json
#  - host-details.json
#  - hosts-overview.json
#  - osd-device-details.json
#  - osds-overview.json
#  - pool-detail.json
#  - pool-overview.json
#  - radosgw-detail.json
#  - radosgw-overview.json
#  - rbd-overview.json
#grafana_plugins:
#  - vonage-status-panel
#  - grafana-piechart-panel
#grafana_allow_embedding: True
#grafana_port: 3000
#prometheus_container_image: "prom/prometheus:v2.7.2"
#prometheus_container_cpu_period: 100000
#prometheus_container_cpu_cores: 2
# container_memory is in GB
#prometheus_container_memory: 4
#prometheus_data_dir: /var/lib/prometheus
#prometheus_conf_dir: /etc/prometheus
#prometheus_user_id: '65534'  # This is the UID used by the prom/prometheus container image
prometheus_port: 9091
#alertmanager_container_image: "prom/alertmanager:v0.16.2"
#alertmanager_container_cpu_period: 100000
#alertmanager_container_cpu_cores: 2
# container_memory is in GB
#alertmanager_container_memory: 4
#alertmanager_data_dir: /var/lib/alertmanager
#alertmanager_conf_dir: /etc/alertmanager
#alertmanager_port: 9093


##########
# CEPHFS #
##########
cephfs_data_pool:
 name: "{{ cephfs_data if cephfs_data is defined else 'cephfs_data' }}"
 pg_num: "128"
 pgp_num: "128"
 rule_name: "replicated_rule"
 type: 1
 erasure_profile: ""
 expected_num_objects: ""
 application: "cephfs"
 size: "{{ osd_pool_default_size }}"
 min_size: "{{ osd_pool_default_min_size }}"
cephfs_metadata_pool:
 name: "{{ cephfs_metadata if cephfs_metadata is defined else 'cephfs_metadata' }}"
 pg_num: "64"
 pgp_num: "64"
 rule_name: "replicated_rule"
 type: 1
 erasure_profile: ""
 expected_num_objects: ""
 application: "cephfs"
 size: "{{ osd_pool_default_size }}"
 min_size: "{{ osd_pool_default_min_size }}"

##########
# TUNING #
##########

# To support buckets with a very large number of objects it's
# important to split them into shards. We suggest about 100K
# objects per shard as a conservative maximum.
# number of objects expected in a bucket / 100,000
#rgw_override_bucket_index_max_shards: 16

# Consider setting a quota on buckets so that exceeding this
# limit will require admin intervention.
#rgw_bucket_default_quota_max_objects: 1638400 # i.e., 100K * 16

# This dictionary will create pools with the given number of pgs.
# This is important because they would be created with the default
# of 8.
# New pools and their corresponding pg_nums can be created
# by adding to the rgw_create_pools dictionary (see foo).
#rgw_create_pools:
#  defaults.rgw.buckets.data:
#    pg_num: 16
#    size: ""
#  defaults.rgw.buckets.index:
#    pg_num: 32
#    size: ""
#  foo:
#    pg_num: 4
#    size: ""

#############
# MULTISITE #
#############
#rgw_multisite: false

# The following Multi-site related variables should be set by the user.
#
# If there is more than 1 RGW in a master or secondary cluster than rgw_multisite_endpoints needs to be a comma seperated list (with no spaces) of the RGW endpoints in the format:
# {{ rgw_multisite_proto }}://{{ ansible_fqdn }}:{{ radosgw_frontend_port }}
# ex: rgw_multisite_endpoints: http://foo.example.com:8080,http://bar.example.com:8080,http://baz.example.com:8080
#
# If there is only 1 RGW in the inventory, rgw_multisite_endpoints does not need to change
#
# rgw_zone is set to "default" to enable compression for clusters configured without rgw multi-site
# If multisite is configured rgw_zone should not be set to "default". See README-MULTISITE.md for an example.
#rgw_zone: default

#rgw_zonemaster: true
#rgw_zonesecondary: false
#rgw_multisite_proto: "http"
#rgw_multisite_endpoint_addr: "{{ ansible_fqdn }}"
#rgw_multisite_endpoints_list: "{{ rgw_multisite_proto }}://{{ ansible_fqdn }}:{{ radosgw_frontend_port }}"
#rgw_zonegroup: solarsystem # should be set by the user
#rgw_zone_user: zone.user
#rgw_realm: milkyway # should be set by the user
#system_access_key: 6kWkikvapSnHyE22P7nO # should be re-created by the user
#system_secret_key: MGecsMrWtKZgngOHZdrd6d3JxGO5CPWgT2lcnpSt # should be re-created by the user

# Multi-site remote pull URL variables
#rgw_pull_port: "{{ radosgw_civetweb_port }}"
#rgw_pull_proto: "http" # should be the same as rgw_multisite_proto for the master zone cluster
#rgw_pullhost: localhost # rgw_pullhost only needs to be declared if there is a zone secondary. It should be the same as rgw_multisite_endpoint_addr for the master zone cluster

